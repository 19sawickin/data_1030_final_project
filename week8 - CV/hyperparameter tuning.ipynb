{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudcard\n",
    "- **How do we find the range for C and gamma for hyperparameter tuning of SVMs? Do we need to visualize the data first?**\n",
    "- **For SVR, how do we know what the width of the gaussian should be? Is it better if the width is as low as possible?**\n",
    "    - it's a hyperparameter (gamma) so as usual, you calculate train and validation scores\n",
    "- **In the case of regression, are SVM's exactly the same as kernel density estimation?**\n",
    "    - it's very similar but the goal is different\n",
    "    - in KDE, your goal is to plot a smooth distribution instead of a histogram\n",
    "    - in SVR rbf, your goal is to predict the regression target variable for previously unseen points\n",
    "  \n",
    "\n",
    "- **Muddiest part was understanding how summing different gaussian functions result in the final prediction function for SVR. Is this summing similar to the Taylor series of a function?**\n",
    "    - nope\n",
    "    - it's quite literally just replacing each point with a gaussian and the model prediction is the sum of the gaussians\n",
    "- **I am still confused how widening the Gaussian predictions creates such a smooth curve for predictions in SVMs.**\n",
    "    - implement the algorithm yourself to figure it out\n",
    "    - it's not too difficult and it's a great exercise to deepen your understanding\n",
    "- **Which library or method would you recommend if we want to check the memory used?**\n",
    "    - as usual, ask [stackoverflow](https://stackoverflow.com/questions/110259/which-python-memory-profiler-is-recommended)\n",
    "- **Could you please post the codes for the quiz 2?**\n",
    "    - once you submit your solution, the code should show up in canvas\n",
    "- **I'm still unclear about the quiz question:¬†The random forest run-time scales linearly with n_samples? I couldn't tell the linear scaling from the graph or the run time values.**\n",
    "    - you might need to average the runtime of multiple fits or use more datapoints to see it well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The supervised ML pipeline\n",
    "The goal: Use the training data (X and y) to develop a <font color='red'>model</font> which can <font color='red'>accurately</font> predict the target variable (y_new') for previously unseen data (X_new).\n",
    "\n",
    "**1. Exploratory Data Analysis (EDA)**: you need to understand your data and verify that it doesn't contain errors\n",
    "   - do as much EDA as you can!\n",
    "    \n",
    "**2. Split the data into different sets**: most often the sets are train, validation, and test (or holdout)\n",
    "   - practitioners often make errors in this step!\n",
    "   - you can split the data randomly, based on groups, based on time, or any other non-standard way if necessary to answer your ML question\n",
    "\n",
    "**3. Preprocess the data**: ML models only work if X and Y are numbers! Some ML models additionally require each feature to have 0 mean and 1 standard deviation (standardized features)\n",
    "   - often the original features you get contain strings (for example a gender feature would contain 'male', 'female', 'non-binary', 'unknown') which needs to transformed into numbers\n",
    "   - often the features are not standardized (e.g., age is between 0 and 100) but it needs to be standardized\n",
    "    \n",
    "**4. Choose an evaluation metric**: depends on the priorities of the stakeholders\n",
    "   - often requires quite a bit of thinking and ethical considerations\n",
    "     \n",
    "**5. Choose one or more ML techniques**: it is highly recommended that you try multiple models\n",
    "   - start with simple models like linear or logistic regression\n",
    "   - try also more complex models like nearest neighbors, support vector machines, random forest, etc.\n",
    "    \n",
    "<span style=\"background-color: #FFFF00\">**6. Tune the hyperparameters of your ML models (aka cross-validation)**</span>\n",
    "   - ML techniques have hyperparameters that you need to optimize to achieve best performance\n",
    "   - for each ML model, decide which parameters to tune and what values to try\n",
    "   - loop through each parameter combination\n",
    "       - train one model for each parameter combination\n",
    "       - evaluate how well the model performs on the validation set\n",
    "   - take the parameter combo that gives the best validation score\n",
    "   - evaluate that model on the test set to report how well the model is expected to perform on previously unseen data\n",
    "    \n",
    "**7. Interpret your model**: black boxes are often not useful\n",
    "   - check if your model uses features that make sense (excellent tool for debugging)\n",
    "   - often model predictions are not enough, you need to be able to explain how the model arrived to a particular prediction (e.g., in health care)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's put everything together\n",
    "- IID data first!\n",
    "- the adult dataset\n",
    "- the next two cells were copied from the week 3 material and slightly rewritten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import packages\n",
    "\n",
    "load your dataset\n",
    "\n",
    "create feature matrix and target variable\n",
    "\n",
    "for i in random_states:\n",
    "\n",
    "   - split the data\n",
    "   - preprocess it\n",
    "   - decide which hyperparameters you'll tune and what values you'll try\n",
    "   - for combo in hyperparameters:\n",
    "       - train your ML algo\n",
    "       - calculate validation scores\n",
    "   - select best model based on the mean and std validation scores\n",
    "   - predict the test set using the best model\n",
    "   - return your test score (generalization error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('data/adult_data.csv')\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['gross-income'] # remember, we want to predict who earns more than 50k or less than 50k\n",
    "X = df.loc[:, df.columns != 'gross-income'] # all other columns are features\n",
    "\n",
    "# collect which encoder to use on each feature\n",
    "# needs to be done manually\n",
    "ordinal_ftrs = ['education'] \n",
    "ordinal_cats = [[' Preschool',' 1st-4th',' 5th-6th',' 7th-8th',' 9th',' 10th',' 11th',' 12th',' HS-grad',\\\n",
    "                ' Some-college',' Assoc-voc',' Assoc-acdm',' Bachelors',' Masters',' Prof-school',' Doctorate']]\n",
    "onehot_ftrs = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "minmax_ftrs = ['age','hours-per-week']\n",
    "std_ftrs = ['capital-gain','capital-loss']\n",
    "\n",
    "# collect all the encoders into one preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', OrdinalEncoder(categories = ordinal_cats), ordinal_ftrs),\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('minmax', MinMaxScaler(), minmax_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "prep = Pipeline(steps=[('preprocessor', preprocessor)]) # for now we only preprocess, later we will add other steps here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "Let's recap preprocessing. Which of these statements are true?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randoms state 1\n",
      "    {'max_features': 0.25, 'max_depth': 1}\n",
      "    0.7599815724815725 0.7581388206388207\n",
      "    {'max_features': 0.5, 'max_depth': 1}\n",
      "    0.7599815724815725 0.7581388206388207\n",
      "    {'max_features': 0.75, 'max_depth': 1}\n",
      "    0.7599815724815725 0.7581388206388207\n",
      "    {'max_features': 1.0, 'max_depth': 1}\n",
      "    0.7599815724815725 0.7581388206388207\n",
      "    {'max_features': 0.25, 'max_depth': 3}\n",
      "    0.8408579033579033 0.8413697788697788\n",
      "    {'max_features': 0.5, 'max_depth': 3}\n",
      "    0.8433149058149059 0.8465909090909091\n",
      "    {'max_features': 0.75, 'max_depth': 3}\n",
      "    0.842956592956593 0.8459766584766585\n",
      "    {'max_features': 1.0, 'max_depth': 3}\n",
      "    0.8421375921375921 0.8456695331695332\n",
      "    {'max_features': 0.25, 'max_depth': 10}\n",
      "    0.8746928746928747 0.8616400491400491\n",
      "    {'max_features': 0.5, 'max_depth': 10}\n",
      "    0.8763308763308764 0.8627149877149877\n",
      "    {'max_features': 0.75, 'max_depth': 10}\n",
      "    0.8761261261261262 0.8614864864864865\n",
      "    {'max_features': 1.0, 'max_depth': 10}\n",
      "    0.8761773136773137 0.8614864864864865\n",
      "    {'max_features': 0.25, 'max_depth': 30}\n",
      "    0.9780917280917281 0.8547297297297297\n",
      "    {'max_features': 0.5, 'max_depth': 30}\n",
      "    0.9797809172809173 0.8541154791154791\n",
      "    {'max_features': 0.75, 'max_depth': 30}\n",
      "    0.9807534807534808 0.850583538083538\n",
      "    {'max_features': 1.0, 'max_depth': 30}\n",
      "    0.9805487305487306 0.8495085995085995\n",
      "    {'max_features': 0.25, 'max_depth': 100}\n",
      "    0.9819819819819819 0.8521191646191646\n",
      "    {'max_features': 0.5, 'max_depth': 100}\n",
      "    0.9819819819819819 0.851044226044226\n",
      "    {'max_features': 0.75, 'max_depth': 100}\n",
      "    0.9819819819819819 0.8511977886977887\n",
      "    {'max_features': 1.0, 'max_depth': 100}\n",
      "    0.9819819819819819 0.8487407862407862\n",
      "best model parameters: {'max_features': 0.5, 'max_depth': 10}\n",
      "corresponding validation score: 0.8627149877149877\n",
      "test score: 0.8624289881774911\n",
      "randoms state 2\n",
      "    {'max_features': 0.25, 'max_depth': 1}\n",
      "    0.7588554463554463 0.7547604422604423\n",
      "    {'max_features': 0.5, 'max_depth': 1}\n",
      "    0.7904381654381655 0.788544226044226\n",
      "    {'max_features': 0.75, 'max_depth': 1}\n",
      "    0.7588554463554463 0.7547604422604423\n",
      "    {'max_features': 1.0, 'max_depth': 1}\n",
      "    0.7588554463554463 0.7547604422604423\n",
      "    {'max_features': 0.25, 'max_depth': 3}\n",
      "    0.8409602784602784 0.836916461916462\n",
      "    {'max_features': 0.5, 'max_depth': 3}\n",
      "    0.8458742833742834 0.8398341523341524\n",
      "    {'max_features': 0.75, 'max_depth': 3}\n",
      "    0.8447481572481572 0.839527027027027\n",
      "    {'max_features': 1.0, 'max_depth': 3}\n",
      "    0.8448505323505323 0.8396805896805897\n",
      "    {'max_features': 0.25, 'max_depth': 10}\n",
      "    0.8752047502047502 0.8567260442260443\n",
      "    {'max_features': 0.5, 'max_depth': 10}\n",
      "    0.8781224406224406 0.8602579852579852\n",
      "    {'max_features': 0.75, 'max_depth': 10}\n",
      "    0.8779176904176904 0.8616400491400491\n",
      "    {'max_features': 1.0, 'max_depth': 10}\n",
      "    0.8778153153153153 0.859490171990172\n",
      "    {'max_features': 0.25, 'max_depth': 30}\n",
      "    0.9798321048321048 0.8485872235872236\n",
      "    {'max_features': 0.5, 'max_depth': 30}\n",
      "    0.9816748566748567 0.8508906633906634\n",
      "    {'max_features': 0.75, 'max_depth': 30}\n",
      "    0.9817772317772318 0.8498157248157249\n",
      "    {'max_features': 1.0, 'max_depth': 30}\n",
      "    0.9816236691236692 0.8487407862407862\n",
      "    {'max_features': 0.25, 'max_depth': 100}\n",
      "    0.9830569205569205 0.8482800982800983\n",
      "    {'max_features': 0.5, 'max_depth': 100}\n",
      "    0.9830569205569205 0.8468980343980343\n",
      "    {'max_features': 0.75, 'max_depth': 100}\n",
      "    0.9830569205569205 0.847512285012285\n",
      "    {'max_features': 1.0, 'max_depth': 100}\n",
      "    0.983005733005733 0.8459766584766585\n",
      "best model parameters: {'max_features': 0.75, 'max_depth': 10}\n",
      "corresponding validation score: 0.8616400491400491\n",
      "test score: 0.8615077537233226\n",
      "randoms state 3\n",
      "    {'max_features': 0.25, 'max_depth': 1}\n",
      "    0.7600839475839476 0.7530712530712531\n",
      "    {'max_features': 0.5, 'max_depth': 1}\n",
      "    0.7705773955773956 0.7627457002457002\n",
      "    {'max_features': 0.75, 'max_depth': 1}\n",
      "    0.7600839475839476 0.7530712530712531\n",
      "    {'max_features': 1.0, 'max_depth': 1}\n",
      "    0.7600839475839476 0.7530712530712531\n",
      "    {'max_features': 0.25, 'max_depth': 3}\n",
      "    0.8442362817362817 0.8353808353808354\n",
      "    {'max_features': 0.5, 'max_depth': 3}\n",
      "    0.846027846027846 0.8379914004914005\n",
      "    {'max_features': 0.75, 'max_depth': 3}\n",
      "    0.8456183456183456 0.8372235872235873\n",
      "    {'max_features': 1.0, 'max_depth': 3}\n",
      "    0.8456183456183456 0.8372235872235873\n",
      "    {'max_features': 0.25, 'max_depth': 10}\n",
      "    0.8738738738738738 0.856418918918919\n",
      "    {'max_features': 0.5, 'max_depth': 10}\n",
      "    0.8778153153153153 0.8593366093366094\n",
      "    {'max_features': 0.75, 'max_depth': 10}\n",
      "    0.8767403767403767 0.859029484029484\n",
      "    {'max_features': 1.0, 'max_depth': 10}\n",
      "    0.8759213759213759 0.8588759213759214\n",
      "    {'max_features': 0.25, 'max_depth': 30}\n",
      "    0.9781941031941032 0.8556511056511057\n",
      "    {'max_features': 0.5, 'max_depth': 30}\n",
      "    0.9801392301392301 0.8541154791154791\n",
      "    {'max_features': 0.75, 'max_depth': 30}\n",
      "    0.9804463554463555 0.8539619164619164\n",
      "    {'max_features': 1.0, 'max_depth': 30}\n",
      "    0.9805999180999181 0.8507371007371007\n",
      "    {'max_features': 0.25, 'max_depth': 100}\n",
      "    0.9813677313677314 0.8542690417690417\n",
      "    {'max_features': 0.5, 'max_depth': 100}\n",
      "    0.9813677313677314 0.8516584766584766\n",
      "    {'max_features': 0.75, 'max_depth': 100}\n",
      "    0.9813165438165438 0.8507371007371007\n",
      "    {'max_features': 1.0, 'max_depth': 100}\n",
      "    0.9813677313677314 0.8482800982800983\n",
      "best model parameters: {'max_features': 0.5, 'max_depth': 10}\n",
      "corresponding validation score: 0.8593366093366094\n",
      "test score: 0.8635037617073545\n",
      "randoms state 4\n",
      "    {'max_features': 0.25, 'max_depth': 1}\n",
      "    0.7657145782145782 0.754914004914005\n",
      "    {'max_features': 0.5, 'max_depth': 1}\n",
      "    0.7657145782145782 0.754914004914005\n",
      "    {'max_features': 0.75, 'max_depth': 1}\n",
      "    0.7657145782145782 0.754914004914005\n",
      "    {'max_features': 1.0, 'max_depth': 1}\n",
      "    0.7657145782145782 0.754914004914005\n",
      "    {'max_features': 0.25, 'max_depth': 3}\n",
      "    0.8441850941850941 0.8347665847665847\n",
      "    {'max_features': 0.5, 'max_depth': 3}\n",
      "    0.8479217854217854 0.8356879606879607\n",
      "    {'max_features': 0.75, 'max_depth': 3}\n",
      "    0.846488533988534 0.8361486486486487\n",
      "    {'max_features': 1.0, 'max_depth': 3}\n",
      "    0.846488533988534 0.836455773955774\n",
      "    {'max_features': 0.25, 'max_depth': 10}\n",
      "    0.877968877968878 0.859490171990172\n",
      "    {'max_features': 0.5, 'max_depth': 10}\n",
      "    0.8811936936936937 0.8584152334152334\n",
      "    {'max_features': 0.75, 'max_depth': 10}\n",
      "    0.8822686322686323 0.8591830466830467\n",
      "    {'max_features': 1.0, 'max_depth': 10}\n",
      "    0.883087633087633 0.8582616707616708\n",
      "    {'max_features': 0.25, 'max_depth': 30}\n",
      "    0.9804463554463555 0.8531941031941032\n",
      "    {'max_features': 0.5, 'max_depth': 30}\n",
      "    0.9817260442260443 0.8495085995085995\n",
      "    {'max_features': 0.75, 'max_depth': 30}\n",
      "    0.9822891072891073 0.8499692874692875\n",
      "    {'max_features': 1.0, 'max_depth': 30}\n",
      "    0.9823402948402948 0.847051597051597\n",
      "    {'max_features': 0.25, 'max_depth': 100}\n",
      "    0.9829545454545454 0.8484336609336609\n",
      "    {'max_features': 0.5, 'max_depth': 100}\n",
      "    0.9829545454545454 0.8476658476658476\n",
      "    {'max_features': 0.75, 'max_depth': 100}\n",
      "    0.9829545454545454 0.8481265356265356\n",
      "    {'max_features': 1.0, 'max_depth': 100}\n",
      "    0.9829545454545454 0.8462837837837838\n",
      "best model parameters: {'max_features': 0.25, 'max_depth': 10}\n",
      "corresponding validation score: 0.859490171990172\n",
      "test score: 0.8582834331337326\n",
      "randoms state 5\n",
      "    {'max_features': 0.25, 'max_depth': 1}\n",
      "    0.756961506961507 0.7590601965601965\n",
      "    {'max_features': 0.5, 'max_depth': 1}\n",
      "    0.7872133497133497 0.7926904176904177\n",
      "    {'max_features': 0.75, 'max_depth': 1}\n",
      "    0.756961506961507 0.7590601965601965\n",
      "    {'max_features': 1.0, 'max_depth': 1}\n",
      "    0.756961506961507 0.7590601965601965\n",
      "    {'max_features': 0.25, 'max_depth': 3}\n",
      "    0.833947583947584 0.8381449631449631\n",
      "    {'max_features': 0.5, 'max_depth': 3}\n",
      "    0.842495904995905 0.8465909090909091\n",
      "    {'max_features': 0.75, 'max_depth': 3}\n",
      "    0.8420864045864046 0.8467444717444718\n",
      "    {'max_features': 1.0, 'max_depth': 3}\n",
      "    0.8420864045864046 0.8468980343980343\n",
      "    {'max_features': 0.25, 'max_depth': 10}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m   \u001b[39m\u001b[38;5;124m'\u001b[39m,params) \n\u001b[1;32m     41\u001b[0m clf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\u001b[38;5;241m*\u001b[39mi,n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# initialize the classifier\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train_prep,y_train) \u001b[38;5;66;03m# fit the model\u001b[39;00m\n\u001b[1;32m     43\u001b[0m models\u001b[38;5;241m.\u001b[39mappend(clf) \u001b[38;5;66;03m# save it\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# calculate train and validation accuracy scores\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d1030/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:476\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    465\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    468\u001b[0m ]\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 476\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d1030/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d1030/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d1030/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d1030/lib/python3.10/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d1030/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/d1030/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let's train a random forest classifier\n",
    "\n",
    "# we will loop through nr_states random states so we will return nr_states test scores and nr_states trained models\n",
    "nr_states = 5\n",
    "test_scores = np.zeros(nr_states)\n",
    "final_models = []\n",
    "\n",
    "# loop through the different random states\n",
    "for i in range(nr_states):\n",
    "    print('randoms state '+str(i+1))\n",
    "\n",
    "    # first split to separate out the training set\n",
    "    X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,random_state=42*i)\n",
    "\n",
    "    # second split to separate out the validation and test sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,random_state=42*i)\n",
    "    \n",
    "    # preprocess the sets\n",
    "    X_train_prep = prep.fit_transform(X_train)\n",
    "    X_val_prep = prep.transform(X_val)\n",
    "    X_test_prep = prep.transform(X_test)\n",
    "\n",
    "    # decide which parameters to tune and what values to try\n",
    "    # the default value of any parameter not specified here will be used\n",
    "    param_grid = {\n",
    "                  'max_depth': [1, 3, 10, 30, 100], # no upper bound so the values are evenly spaced in log\n",
    "                  'max_features': [0.25, 0.5,0.75,1.0] # linearly spaced because it is between 0 and 1, 0 is omitted\n",
    "                  } \n",
    "\n",
    "    # we save the train and validation scores\n",
    "    # the validation scores are necessary to select the best model\n",
    "    # it's optional to save the train scores, it can be used to identify high bias and high variance models\n",
    "    train_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    val_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    models = []\n",
    "    \n",
    "    # loop through all combinations of hyperparameter combos\n",
    "    for p in range(len(ParameterGrid(param_grid))):\n",
    "        params = ParameterGrid(param_grid)[p]\n",
    "        print('   ',params) \n",
    "        clf = RandomForestClassifier(**params,random_state = 42*i,n_jobs=-1) # initialize the classifier\n",
    "        clf.fit(X_train_prep,y_train) # fit the model\n",
    "        models.append(clf) # save it\n",
    "        # calculate train and validation accuracy scores\n",
    "        y_train_pred = clf.predict(X_train_prep)\n",
    "        train_score[p] = accuracy_score(y_train,y_train_pred)\n",
    "        y_val_pred = clf.predict(X_val_prep)\n",
    "        val_score[p] = accuracy_score(y_val,y_val_pred)\n",
    "        print('   ',train_score[p],val_score[p])\n",
    "    \n",
    "    # print out model parameters that maximize validation accuracy\n",
    "    print('best model parameters:',ParameterGrid(param_grid)[np.argmax(val_score)])\n",
    "    print('corresponding validation score:',np.max(val_score))\n",
    "    # collect and save the best model\n",
    "    final_models.append(models[np.argmax(val_score)])\n",
    "    # calculate and save the test score\n",
    "    y_test_pred = final_models[-1].predict(X_test_prep)\n",
    "    test_scores[i] = accuracy_score(y_test,y_test_pred)\n",
    "    print('test score:',test_scores[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to look out for\n",
    "- are the ranges of the hyperparameters wide enough?\n",
    "    - if you are unsure, save the training scores and plot the train and val scores!\n",
    "    - do you see underfitting? model performs poorly on both training and validation sets?\n",
    "    - do you see overfitting? model performs very good on training but worse on validation?\n",
    "    - if you don't see both, expand the range of the parameters and you'll likely find a better model\n",
    "    - read the manual and make sure you understand what the hyperparameter does in the model\n",
    "        - some parameters (like regularization parameters) should be evenly spaced in log because there is no upper bound\n",
    "        - some parameters (like max_features) should be linearly spaced because they have clear lower and upper bounds\n",
    "    - if the best hyperparameter is at the edge of your range, you definitely need to expand the range if you can\n",
    "- not every hyperparameter is equally important\n",
    "    - some parameters have little to no impact on train and validation scores\n",
    "    - in the example above, max_depth is much more important than max_features\n",
    "    - visualize the results if in doubt\n",
    "- is the best validation score similar to the test score?\n",
    "    - it's usual that the validation score is a bit better than the test score\n",
    "    - but if the difference between the two scores is significant over multiple random states, something could be off\n",
    "- traiv/val/test split is usually a safe bet for any splitting strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with folds\n",
    "- the steps are a bit different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_pipeline\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/adult_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# let's separate the feature matrix X, and target variable y\u001b[39;00m\n\u001b[1;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgross-income\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# remember, we want to predict who earns more than 50k or less than 50k\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "df = pd.read_csv('data/adult_data.csv')\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['gross-income'] # remember, we want to predict who earns more than 50k or less than 50k\n",
    "X = df.loc[:, df.columns != 'gross-income'] # all other columns are features\n",
    "\n",
    "ordinal_ftrs = ['education'] \n",
    "ordinal_cats = [[' Preschool',' 1st-4th',' 5th-6th',' 7th-8th',' 9th',' 10th',' 11th',' 12th',' HS-grad',\\\n",
    "                ' Some-college',' Assoc-voc',' Assoc-acdm',' Bachelors',' Masters',' Prof-school',' Doctorate']]\n",
    "onehot_ftrs = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "minmax_ftrs = ['age','hours-per-week']\n",
    "std_ftrs = ['capital-gain','capital-loss']\n",
    "\n",
    "# collect all the encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', OrdinalEncoder(categories = ordinal_cats), ordinal_ftrs),\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('minmax', MinMaxScaler(), minmax_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "# all the same up to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use GridSearchCV and the parameter names need to contain the ML algorithm you want to use\n",
    "# the parameters of some ML algorithms have the same name and this is how we avoid confusion\n",
    "param_grid = {\n",
    "              'randomforestclassifier__max_depth': [1, 3, 10, 30, 100], # the max_depth should be smaller or equal than the number of features roughly\n",
    "              'randomforestclassifier__max_features': [0.5,0.75,1.0] # linearly spaced between 0.5 and 1\n",
    "              } \n",
    "\n",
    "nr_states = 3\n",
    "test_scores = np.zeros(nr_states)\n",
    "final_models = []\n",
    "\n",
    "for i in range(nr_states):\n",
    "    # first split to separate out the test set\n",
    "    # we will use kfold on other\n",
    "    X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,random_state=42*i)\n",
    "\n",
    "    # splitter for other\n",
    "    kf = KFold(n_splits=4,shuffle=True,random_state=42*i)\n",
    "\n",
    "    # the classifier\n",
    "    clf = RandomForestClassifier(random_state = 42*i) # initialize the classifier\n",
    "\n",
    "    # let's put together a pipeline\n",
    "    # the pipeline will fit_transform the training set (3 folds), and transform the last fold used as validation\n",
    "    # then it will train the ML algorithm on the training set and evaluate it on the validation set\n",
    "    # it repeats this step automatically such that each fold will be an evaluation set once\n",
    "    pipe = make_pipeline(preprocessor,clf)\n",
    "\n",
    "    # use GridSearchCV\n",
    "    # GridSearchCV loops through all parameter combinations and collects the results \n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid,scoring = 'accuracy',\n",
    "                        cv=kf, return_train_score = True, n_jobs=-1, verbose=True)\n",
    "    \n",
    "    # this line actually fits the model on other\n",
    "    grid.fit(X_other, y_other)\n",
    "    # save results into a data frame. feel free to print it and inspect it\n",
    "    results = pd.DataFrame(grid.cv_results_)\n",
    "    #print(results)\n",
    "\n",
    "    print('best model parameters:',grid.best_params_)\n",
    "    print('validation score:',grid.best_score_) # this is the mean validation score over all iterations\n",
    "    # save the model\n",
    "    final_models.append(grid)\n",
    "    # calculate and save the test score\n",
    "    y_test_pred = final_models[-1].predict(X_test)\n",
    "    test_scores[i] = accuracy_score(y_test,y_test_pred)\n",
    "    print('test score:',test_scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to look out for\n",
    "- less code but more stuff is going on in the background hidden from you\n",
    "    - looping over multiple folds\n",
    "    - .fit_transform and .transform is hidden from you\n",
    "- nevertheless, GridSearchCV and pipelines are pretty powerful\n",
    "- working with folds is a bit more robust because the best hyperparameter is selected based on the average score of multiple trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "Can we use GridSearchCV with sets prepared by train_test_split in advance? Use the sklearn manual or stackoverflow to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mud card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
